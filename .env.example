# Ollama Configuration (Local LLM - No API key needed!)
# This project uses Ollama for all NLP tasks and chatbot features

# Ollama Server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Setup Instructions:
# 1. Download Ollama from: https://ollama.ai/download
# 2. Add to PATH: $env:PATH += ';$env:LOCALAPPDATA\Programs\Ollama'
# 3. Pull a model: ollama pull llama2
# 4. Verify: ollama list
# 5. Run: python main_phase2.py

# Available Models:
# - llama2 (4GB) - Fast, good quality
# - llama3.1 (7GB) - Best quality (recommended)
# - mistral (4GB) - Balanced option

# To use a different model, change it in src/nlp/brochure_generator.py
