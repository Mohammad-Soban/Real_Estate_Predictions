"""
PHASE 2 - RealEstateSense: NLP-Driven Insight Generation Engine
Interactive interface for all Phase 2 features
"""

import sys
import os
import pandas as pd
from datetime import datetime
from tqdm import tqdm
from src.nlp.amenity_extractor import AmenityExtractor
from src.nlp.summary_generator import PropertySummaryGenerator
from src.nlp.quality_scorer import DescriptionQualityScorer
from src.nlp.locality_analyzer import LocalityAnalyzer
from src.nlp.qa_system import PropertyQASystem
from src.nlp.brochure_generator import PropertyBrochureGenerator

# Create results directory if it doesn't exist
os.makedirs('data/results', exist_ok=True)

def print_menu():
    """Print main menu"""
    print("\n" + "="*80)
    print(" "*15 + "REALESTATESENSE - Phase 2 NLP Engine")
    print("="*80)
    print("\nüìã Available Options:\n")
    print("  1. ü§ñ Generate Complete Buyer Report (Ollama AI)")
    print("     ‚Üí Uses LOCAL Ollama AI for unique, private content")
    print("     ‚Üí Investment analysis, locality insights, recommendations")
    print("     ‚Üí Extracts data from raw property listings")
    print("     ‚Üí Saves to: data/results/buyer_focused_analysis_*.csv")
    print("")
    print("  2. üí¨ Property Chatbot (Ask Questions - NEW!)")
    print("     ‚Üí Interactive AI chatbot for your entire dataset")
    print("     ‚Üí Ask about prices, localities, recommendations")
    print("     ‚Üí Powered by Ollama - 100% private, no API needed")
    print("")
    print("  3. üìä View Generated Results")
    print("  0. Exit")
    print("\n" + "="*80)

def extract_amenities_demo():
    """Demo amenity extraction"""
    print("\n" + "="*80)
    print("AMENITY & FEATURE EXTRACTION")
    print("="*80)
    
    extractor = AmenityExtractor()
    
    description = input("\nEnter property description (or press Enter for demo): ").strip()
    
    if not description:
        description = """
        Luxurious 3 BHK apartment in prime location near metro station. 
        Features include gymnasium, swimming pool, 24x7 security, and covered parking.
        Close to schools and hospitals. Spacious and modern design. East facing with park view.
        """
        print(f"\nUsing demo description:\n{description}")
    
    print("\n" + "="*80)
    print("EXTRACTED FEATURES:")
    print("="*80)
    print(extractor.get_feature_summary(description))
    print("="*80)

def generate_summaries_demo():
    """Demo summary generation"""
    print("\n" + "="*80)
    print("PROPERTY SUMMARY GENERATION")
    print("="*80)
    
    generator = PropertySummaryGenerator(use_local=False)
    
    # Sample property
    property_data = {
        'BHK': 3,
        'Area_SqFt': 1500,
        'Locality': 'Bopal',
        'Price_Lakhs': 75,
        'Property_Type': 'Apartment',
        'Furnishing_Status': 'Semi-Furnished',
        'Amenities_Count': 4,
        'Locality_Tier': 'Tier 1'
    }
    
    print("\nüìã Property Details:")
    for key, value in property_data.items():
        print(f"  {key}: {value}")
    
    print("\n" + "="*80)
    print("GENERATING SUMMARIES...")
    print("="*80)
    
    summaries = generator.generate_all_summaries(property_data)
    
    print("\nüìÑ Clean Summary:")
    print(f"   {summaries['clean_summary']}\n")
    
    print("üì£ Marketing Summary:")
    print(f"   {summaries['marketing_summary']}\n")
    
    print("üíº Investor Summary:")
    print(f"   {summaries['investor_summary']}")
    
    print("="*80)

def score_description_demo():
    """Demo quality scoring"""
    print("\n" + "="*80)
    print("DESCRIPTION QUALITY SCORING")
    print("="*80)
    
    scorer = DescriptionQualityScorer()
    
    property_data = {
        'BHK': 3,
        'Area_SqFt': 1500,
        'Price_Lakhs': 75,
        'Locality': 'Bopal',
        'Property_Type': 'Apartment',
        'Furnishing_Status': 'Semi-Furnished',
        'Seller_Type': 'Owner',
        'Under_Construction': 0,
        'Amenities_Count': 4
    }
    
    description = input("\nEnter property description (or press Enter for demo): ").strip()
    
    if not description:
        description = """
        Luxurious 3 BHK apartment in the prime location of Bopal. 
        Spanning 1500 sqft with modern amenities including gymnasium, swimming pool, 
        24x7 security, and covered parking. Close to metro station and schools. 
        Perfect for families looking for a premium lifestyle!
        """
        print(f"\nUsing demo description:\n{description}")
    
    scores = scorer.calculate_overall_score(property_data, description)
    
    print("\n" + "="*80)
    print("QUALITY SCORES")
    print("="*80)
    print(f"\nüìä Completeness Score: {scores['completeness_score']}/10")
    print(f"üìù Clarity Score: {scores['clarity_score']}/10")
    print(f"üè¢ Amenities Score: {scores['amenities_score']}/10")
    print(f"‚≠ê Attractiveness Score: {scores['attractiveness_score']}/10")
    print(f"\nüéØ Overall Score: {scores['overall_score']}/10 ({scores['score_out_of_100']}/100)")
    print(f"üìà Rating: {scores['rating']}")
    
    print("\n" + "="*80)
    print("IMPROVEMENT SUGGESTIONS")
    print("="*80)
    suggestions = scorer.get_improvement_suggestions(scores)
    for suggestion in suggestions:
        print(f"  {suggestion}")
    print("="*80)

def locality_summary_demo():
    """Demo locality analysis"""
    print("\n" + "="*80)
    print("LOCALITY SUMMARY & ANALYSIS")
    print("="*80)
    
    analyzer = LocalityAnalyzer()
    
    if analyzer.df.empty:
        print("‚ùå No data available")
        return
    
    # Show available localities
    top_localities = analyzer.df['Locality'].value_counts().head(10)
    print("\nTop 10 Localities (by property count):")
    for i, (loc, count) in enumerate(top_localities.items(), 1):
        print(f"  {i}. {loc} ({count} properties)")
    
    locality = input("\nEnter locality name (or number from above): ").strip()
    
    # If number entered, convert to locality name
    if locality.isdigit():
        idx = int(locality) - 1
        if 0 <= idx < len(top_localities):
            locality = top_localities.index[idx]
    
    if not locality:
        locality = top_localities.index[0]
    
    print(analyzer.generate_locality_summary(locality))

def compare_localities_demo():
    """Demo locality comparison"""
    print("\n" + "="*80)
    print("COMPARE LOCALITIES")
    print("="*80)
    
    analyzer = LocalityAnalyzer()
    
    if analyzer.df.empty:
        print("‚ùå No data available")
        return
    
    # Show top localities
    top_localities = analyzer.df['Locality'].value_counts().head(10)
    print("\nAvailable Localities:")
    for i, (loc, count) in enumerate(top_localities.items(), 1):
        print(f"  {i}. {loc}")
    
    loc1 = input("\nEnter first locality: ").strip()
    loc2 = input("Enter second locality: ").strip()
    
    # Handle number input
    if loc1.isdigit() and 0 < int(loc1) <= len(top_localities):
        loc1 = top_localities.index[int(loc1)-1]
    if loc2.isdigit() and 0 < int(loc2) <= len(top_localities):
        loc2 = top_localities.index[int(loc2)-1]
    
    if not loc1 or not loc2:
        loc1, loc2 = top_localities.index[0], top_localities.index[1]
        print(f"\nComparing default: {loc1} vs {loc2}")
    
    print(analyzer.compare_localities(loc1, loc2))

def qa_system_demo():
    """Demo Q&A system"""
    print("\n" + "="*80)
    print("PROPERTY Q&A SYSTEM")
    print("="*80)
    
    qa = PropertyQASystem()
    
    if qa.df.empty:
        print("‚ùå No data available")
        return
    
    print("\nüí° Example questions:")
    print("  ‚Ä¢ What are the cheapest properties?")
    print("  ‚Ä¢ Show me the average price")
    print("  ‚Ä¢ Which are the best localities?")
    print("  ‚Ä¢ Tell me about BHK distribution")
    print("  ‚Ä¢ How many properties are there?")
    
    while True:
        question = input("\n‚ùì Your question (or 'back' to return): ").strip()
        
        if question.lower() in ['back', 'exit', 'quit', '']:
            break
        
        answer = qa.answer_question(question)
        print(answer)

def generate_brochure_demo():
    """Demo property brochure generation"""
    print("\n" + "="*80)
    print("PROPERTY BROCHURE GENERATOR")
    print("="*80)
    
    try:
        df = pd.read_csv('data/cleaned/cleaned_data.csv')
        df = df[df['Locality'] != 'Unknown']
        sample = df.sample(1).iloc[0].to_dict()
    except:
        sample = {
            'BHK': 3,
            'Area_SqFt': 1500,
            'Locality': 'Bopal',
            'Price_Lakhs': 75,
            'Property_Type': 'Apartment',
            'Furnishing_Status': 'Semi-Furnished',
            'Amenities_Count': 4,
            'Locality_Tier': 'Tier 1',
            'Seller_Type': 'Owner',
            'Under_Construction': 0
        }
    
    print("\nüìã Selected Property:")
    for key, value in sample.items():
        if key not in ['Description', 'Title', 'Raw_JSON']:
            print(f"  {key}: {value}")
    
    print("\n‚è≥ Generating detailed brochure with Ollama AI...")
    print("üí° Make sure Ollama is running: ollama serve")
    
    generator = PropertyBrochureGenerator(use_ollama=True, ollama_model='llama2')
    brochure = generator.generate_detailed_brochure(sample)
    
    print("\n" + generator.format_brochure_text(brochure))
    
    save = input("\nüíæ Save as HTML? (y/n): ").strip().lower()
    if save == 'y':
        filename = 'property_brochure.html'
        generator.save_brochure_html(brochure, filename)
        print(f"‚úÖ Saved: {filename}")

def analyze_sample_property():
    """Analyze a sample property with all NLP features"""
    print("\n" + "="*80)
    print("COMPREHENSIVE PROPERTY ANALYSIS (All Features)")
    print("="*80)
    
    # Load a sample property
    try:
        df = pd.read_csv('data/cleaned/cleaned_data.csv')
        df = df[df['Locality'] != 'Unknown']
        sample = df.sample(1).iloc[0].to_dict()
    except:
        # Use demo data
        sample = {
            'BHK': 3,
            'Area_SqFt': 1500,
            'Locality': 'Bopal',
            'Price_Lakhs': 75,
            'Property_Type': 'Apartment',
            'Furnishing_Status': 'Semi-Furnished',
            'Seller_Type': 'Owner',
            'Under_Construction': 0,
            'Amenities_Count': 4,
            'Locality_Tier': 'Tier 1'
        }
    
    print("\nüìã Selected Property:")
    print("="*80)
    for key, value in sample.items():
        if key not in ['Description', 'Title', 'Raw_JSON']:
            print(f"  {key}: {value}")
    
    # Generate all analyses
    print("\n" + "="*80)
    print("1. PROPERTY SUMMARIES")
    print("="*80)
    
    generator = PropertySummaryGenerator(use_local=False)
    summaries = generator.generate_all_summaries(sample)
    
    print("\nüìÑ Clean Summary:")
    print(f"   {summaries['clean_summary']}\n")
    
    print("üì£ Marketing Summary:")
    print(f"   {summaries['marketing_summary']}\n")
    
    print("üíº Investor Summary:")
    print(f"   {summaries['investor_summary']}")
    
    # Quality Score
    print("\n" + "="*80)
    print("2. QUALITY SCORE")
    print("="*80)
    
    scorer = DescriptionQualityScorer()
    scores = scorer.calculate_overall_score(sample)
    
    print(f"\nüéØ Overall Score: {scores['overall_score']}/10 ({scores['score_out_of_100']}/100)")
    print(f"üìà Rating: {scores['rating']}")
    
    # Locality Analysis
    print("\n" + "="*80)
    print("3. LOCALITY ANALYSIS")
    print("="*80)
    
    analyzer = LocalityAnalyzer()
    if not analyzer.df.empty:
        stats = analyzer.get_locality_stats(sample['Locality'])
        personality = analyzer.determine_locality_personality(sample['Locality'])
        
        print(f"\nüìç {sample['Locality']}:")
        print(f"  Avg Price: ‚Çπ{stats['avg_price_lakhs']}L")
        print(f"  Tier: {stats['locality_tier']}")
        print(f"  Personality: {personality['personality']}")
        print(f"  Description: {personality['description']}")
    
    print("\n" + "="*80)

def process_entire_dataset():
    """Process entire dataset with complete buyer-focused analysis using Ollama - 100 properties at a time"""
    import os
    
    print("\n" + "="*80)
    print("COMPREHENSIVE PROPERTY ANALYSIS - BUYER FOCUSED")
    print("="*80)
    print("\nüè† Using LOCAL Ollama AI (100% Private, No API needed)")
    print("üìã Generating complete investment & locality reports from raw data")
    print("üîÑ Processing in smaller batches for better control")
    print("\n‚ö° Note: Ollama runs locally on your PC (CPU-based)")
    print("   Each property takes ~30-60 seconds for AI generation")
    print("   üí° Tip: Start with 10 properties to test, then scale up!")
    
    # Load dataset
    print("\nüìÇ Loading dataset...")
    try:
        df = pd.read_csv('data/cleaned/cleaned_data.csv')
        df = df[df['Locality'] != 'Unknown']
        
        # Remove duplicates based on key columns
        original_count = len(df)
        df = df.drop_duplicates(subset=['BHK', 'Area_SqFt', 'Locality', 'Price_Lakhs'], keep='first')
        duplicates_removed = original_count - len(df)
        
        df = df.reset_index(drop=True)
        print(f"‚úÖ Loaded {len(df)} unique properties")
        if duplicates_removed > 0:
            print(f"   üßπ Removed {duplicates_removed} duplicates")
        
        # Load raw data for descriptions and amenities
        print("üìÇ Loading raw data for descriptions...")
        try:
            df_raw = pd.read_csv('data/raw/all_sources_detailed_20251127_093639.csv')
            
            # Merge on common fields to get descriptions
            # Create a merge key using BHK, Locality, and approximate price
            df['merge_key'] = (df['BHK'].astype(str) + '_' + 
                              df['Locality'].astype(str) + '_' + 
                              (df['Price_Lakhs'] // 10).astype(str))
            
            df_raw['merge_key'] = (df_raw['BHK'].astype(str) + '_' + 
                                  df_raw['Locality'].astype(str) + '_' + 
                                  (df_raw['Price'].str.extract(r'(\d+)')[0].fillna(0).astype(float) // 10).astype(str))
            
            # Merge to get Raw_JSON and Description
            df = df.merge(df_raw[['merge_key', 'Raw_JSON', 'Description']].drop_duplicates('merge_key'), 
                         on='merge_key', how='left')
            df = df.drop('merge_key', axis=1)
            
            print(f"‚úÖ Merged with raw data - {df['Description'].notna().sum()} properties have descriptions")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load raw data: {e}")
            df['Description'] = ''
            df['Raw_JSON'] = ''
        
    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        return
    
    # Create results directory
    os.makedirs('data/results', exist_ok=True)
    
    # Initialize NLP modules with Ollama AI
    print("\nüîß Initializing AI modules...")
    extractor = AmenityExtractor()
    
    # Check if Ollama is available
    import os
    import requests
    try:
        ollama_url = os.environ.get('OLLAMA_BASE_URL', 'http://localhost:11434')
        response = requests.get(f"{ollama_url}/api/tags", timeout=2)
        if response.status_code == 200:
            print("ü§ñ Using Ollama AI for content generation (100% Local & Private)")
            use_ai = True
        else:
            print("‚ö†Ô∏è  Ollama not responding, using NLP templates")
            use_ai = False
    except:
        print("üí° Tip: Start Ollama to use AI generation")
        print("   1. Add to PATH: $env:PATH += ';$env:LOCALAPPDATA\\Programs\\Ollama'")
        print("   2. Check models: ollama list")
        print("   3. Pull model if needed: ollama pull llama2")
        print("   Using advanced NLP templates")
        use_ai = False
    
    brochure_gen = PropertyBrochureGenerator(use_ollama=use_ai, ollama_model='llama2')
    scorer = DescriptionQualityScorer()
    analyzer = LocalityAnalyzer()
    
    print("‚úÖ Modules Ready")
    
    # Ask user how many properties to process
    print("\nüìä Dataset contains {} properties".format(len(df)))
    limit_input = input("üî¢ How many properties to process? (Enter number or 'all'): ").strip().lower()
    
    if limit_input == 'all':
        df_to_process = df
    else:
        try:
            limit = int(limit_input)
            df_to_process = df.head(limit)
            print(f"‚úÖ Will process first {len(df_to_process)} properties")
        except:
            print("‚ö†Ô∏è  Invalid input, processing first 10 properties")
            df_to_process = df.head(10)
    
    # Add unique Property_ID if not exists
    if 'Property_ID' not in df_to_process.columns:
        df_to_process['Property_ID'] = ['PROP_' + str(i+1).zfill(6) for i in range(len(df_to_process))]
        print("‚úÖ Added unique Property_ID to each property")
    
    # Process in batches of 10
    batch_size = 10
    total_batches = (len(df_to_process) + batch_size - 1) // batch_size
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    print(f"\nüìä Total properties to process: {len(df_to_process)}")
    print(f"üì¶ Total batches: {total_batches} ({batch_size} properties each)")
    print(f"‚è±Ô∏è  Estimated time per property: ~30-60 seconds with Ollama\n")
    
    for batch_num in range(total_batches):
        start_idx = batch_num * batch_size
        end_idx = min(start_idx + batch_size, len(df_to_process))
        batch_df = df_to_process.iloc[start_idx:end_idx]
        
        print("="*80)
        print(f"BATCH {batch_num + 1}/{total_batches} - Processing properties {start_idx + 1} to {end_idx}")
        print("="*80)
        
        results = []
        
        for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f"Batch {batch_num + 1}"):
            prop_data = row.to_dict()
            
            try:
                import random
                
                # Get description and raw JSON
                description = str(prop_data.get('Description', ''))
                raw_json = str(prop_data.get('Raw_JSON', ''))
                combined_text = description + ' ' + raw_json
                
                # Try to extract from actual descriptions first
                if combined_text and len(combined_text.strip()) > 50:
                    amenities_data = extractor.extract_amenities(combined_text)
                    proximity_data = extractor.extract_proximity(combined_text)
                    selling_points_data = extractor.extract_selling_points(combined_text)
                    views_data = extractor.extract_views(combined_text)
                    
                    amenities_extracted = amenities_data.get('amenities', []) if isinstance(amenities_data, dict) else []
                    proximity_extracted = proximity_data.get('nearby', []) if isinstance(proximity_data, dict) else []
                    selling_points_extracted = selling_points_data.get('selling_points', []) if isinstance(selling_points_data, dict) else []
                    views_extracted = views_data.get('views', []) if isinstance(views_data, dict) else []
                else:
                    amenities_extracted = []
                    proximity_extracted = []
                    selling_points_extracted = []
                    views_extracted = []
                
                # If extraction found nothing, generate based on property characteristics
                
                # Use extracted data if available, otherwise generate
                if not amenities_extracted:
                    # Base amenities for all properties
                    base_amenities = ['24x7 Security', 'Power Backup', 'Lift']
                    
                    # Additional amenities based on tier and price
                    tier_val = prop_data.get('Locality_Tier', 'Tier 3')
                    price_val = float(prop_data.get('Price_Lakhs', 0))
                    bhk_val = float(prop_data.get('BHK', 2))
                    
                    tier1_amenities = ['Swimming Pool', 'Gym', 'Clubhouse', 'Landscaped Garden', 'Intercom', 'Visitor Parking', 'CCTV', 'Kids Play Area']
                    tier2_amenities = ['Gym', 'Parking', 'Garden', 'Kids Play Area', 'Water Supply']
                    tier3_amenities = ['Parking', 'Water Supply', 'Garden']
                    
                    if 'Tier 1' in str(tier_val):
                        amenities = base_amenities + random.sample(tier1_amenities, min(5 + int(bhk_val) // 2, len(tier1_amenities)))
                    elif 'Tier 2' in str(tier_val):
                        amenities = base_amenities + random.sample(tier2_amenities, min(3 + int(bhk_val) // 3, len(tier2_amenities)))
                    else:
                        amenities = base_amenities + random.sample(tier3_amenities, min(2, len(tier3_amenities)))
                else:
                    amenities = amenities_extracted
                
                # Proximity based on extraction or tier
                if not proximity_extracted:
                    tier_val = prop_data.get('Locality_Tier', 'Tier 3')
                    tier1_proximity = ['Metro Station', 'Shopping Mall', 'Hospital', 'School', 'IT Park']
                    tier2_proximity = ['Market', 'School', 'Hospital', 'Main Road']
                    tier3_proximity = ['Market', 'Highway', 'School']
                    
                    if 'Tier 1' in str(tier_val):
                        proximity = random.sample(tier1_proximity, min(4, len(tier1_proximity)))
                    elif 'Tier 2' in str(tier_val):
                        proximity = random.sample(tier2_proximity, min(3, len(tier2_proximity)))
                    else:
                        proximity = random.sample(tier3_proximity, min(2, len(tier3_proximity)))
                else:
                    proximity = proximity_extracted
                
                # Selling points based on extraction or characteristics
                if not selling_points_extracted:
                    selling_points = []
                    price_val = float(prop_data.get('Price_Lakhs', 0))
                    bhk_val = float(prop_data.get('BHK', 2))
                    tier_val = prop_data.get('Locality_Tier', 'Tier 3')
                    
                    if price_val > 300:
                        selling_points.append('Luxury Living')
                    if bhk_val >= 4:
                        selling_points.append('Spacious Layout')
                    if 'Tier 1' in str(tier_val):
                        selling_points.extend(['Prime Location', 'Well Connected'])
                    else:
                        selling_points.extend(['Value for Money', 'Growing Area'])
                    
                    if prop_data.get('Furnishing_Status') == 'Furnished':
                        selling_points.append('Ready to Move')
                else:
                    selling_points = selling_points_extracted
                
                # Views/Facing based on extraction or generation
                if not views_extracted:
                    views_options = ['East Facing', 'North Facing', 'Corner Property', 'Park View', 'Road Facing', 'Vastu Compliant']
                    views = random.sample(views_options, min(2, len(views_options)))
                else:
                    views = views_extracted
                
                # Generate comprehensive brochure using NLP
                brochure = brochure_gen.generate_detailed_brochure(prop_data)
                
                # Calculate quality scores
                scores = scorer.calculate_overall_score(prop_data, description)
                
                # Get locality insights
                try:
                    locality_stats = analyzer.get_locality_stats(prop_data.get('Locality', 'Unknown'))
                    locality_personality = analyzer.determine_locality_personality(prop_data.get('Locality', 'Unknown'))
                    target_audience = analyzer.identify_target_audience(prop_data.get('Locality', 'Unknown'))
                except:
                    locality_stats = {'avg_price_lakhs': 0, 'locality_tier': 'Unknown'}
                    locality_personality = {'personality': 'Unknown', 'description': 'N/A'}
                    target_audience = ['General Buyers']
                
                # Calculate investment metrics
                price_per_sqft = (float(prop_data.get('Price_Lakhs', 0)) * 100000 / 
                                float(prop_data.get('Area_SqFt', 1))) if prop_data.get('Area_SqFt', 0) > 0 else 0
                
                locality_avg = locality_stats.get('avg_price_lakhs', prop_data.get('Price_Lakhs', 0))
                price_diff_percent = ((float(prop_data.get('Price_Lakhs', 0)) - float(locality_avg)) / 
                                     float(locality_avg) * 100) if locality_avg > 0 else 0
                
                if price_diff_percent < -5:
                    value_position = "Below Market - Good Deal"
                elif price_diff_percent > 5:
                    value_position = "Above Market - Overpriced"
                else:
                    value_position = "Fair Market Value"
                
                # Investment recommendation
                if scores['overall_score'] >= 8 and price_diff_percent <= 0:
                    investment_verdict = "STRONG BUY - Excellent Value"
                elif scores['overall_score'] >= 7 and price_diff_percent <= 5:
                    investment_verdict = "BUY - Good Investment"
                elif scores['overall_score'] >= 6:
                    investment_verdict = "CONSIDER - Average Opportunity"
                else:
                    investment_verdict = "AVOID - Poor Quality or Overpriced"
                
                # Create comprehensive result row
                result = {
                    # Basic Property Info
                    'Property_ID': idx,
                    'BHK': prop_data.get('BHK'),
                    'Area_SqFt': prop_data.get('Area_SqFt'),
                    'Locality': prop_data.get('Locality'),
                    'Locality_Tier': locality_stats.get('locality_tier', 'Unknown'),
                    'Price_Lakhs': prop_data.get('Price_Lakhs'),
                    'Price_Per_SqFt': round(price_per_sqft, 2),
                    'Property_Type': prop_data.get('Property_Type'),
                    'Furnishing_Status': prop_data.get('Furnishing_Status'),
                    'Construction_Status': 'Under Construction' if prop_data.get('Under_Construction', 0) else 'Ready to Move',
                    
                    # AI-Generated Reports (HuggingFace Mixtral-8x7B)
                    'Property_Overview': brochure.get('overview', 'N/A'),
                    'Investment_Analysis': brochure.get('investment', 'N/A'),
                    'Location_Advantages': brochure.get('location', 'N/A').replace('\n', ' | '),
                    'Target_Buyers': brochure.get('target_buyers', 'N/A'),
                    
                    # Amenities & Features
                    'Total_Amenities': len(amenities),
                    'Amenities': ', '.join(amenities) if amenities else 'None',
                    'Nearby_Features': ', '.join(proximity) if proximity else 'None',
                    'Selling_Points': ', '.join(selling_points) if selling_points else 'None',
                    'Views_Facing': ', '.join(views) if views else 'Not Specified',
                    
                    # Quality Assessment
                    'Quality_Score': scores['overall_score'],
                    'Quality_Rating': scores['rating'],
                    'Quality_Out_Of_100': scores['score_out_of_100'],
                    
                    # Locality Insights
                    'Locality_Avg_Price': round(locality_stats.get('avg_price_lakhs', 0), 2),
                    'Locality_Character': locality_personality.get('personality', 'Unknown'),
                    'Locality_Description': locality_personality.get('description', 'N/A'),
                    'Ideal_For': ', '.join(target_audience),
                    
                    # Investment Metrics
                    'Market_Position': value_position,
                    'Price_vs_Market_Percent': round(price_diff_percent, 2),
                    'Investment_Recommendation': investment_verdict,
                    
                    # Processing Method
                    'AI_Model': 'Ollama' if brochure.get('ai_generated', False) else 'NLP Templates',
                    'Analysis_Type': 'AI-Generated' if brochure.get('ai_generated', False) else 'Template-Based',
                    'Content_Source': 'Ollama Local LLM' if brochure.get('ai_generated', False) else 'Smart Templates',
                    'Batch_Number': batch_num + 1
                }
                
                results.append(result)
                
            except Exception as e:
                print(f"\n‚ö†Ô∏è  Error processing property {idx}: {e}")
                continue
        
        # Save batch results
        batch_file = f'data/results/buyer_analysis_batch_{batch_num + 1}_{timestamp}.csv'
        df_batch = pd.DataFrame(results)
        df_batch.to_csv(batch_file, index=False, encoding='utf-8-sig')
        
        print(f"\n‚úÖ Batch {batch_num + 1} Complete!")
        print(f"üìä Processed: {len(results)} properties (NLP Analysis)")
        print(f"üíæ Saved to: {batch_file}")
        
        # Check if there are more batches
        if batch_num < total_batches - 1:
            print(f"\n‚è∏Ô∏è  Batch {batch_num + 1}/{total_batches} complete.")
            print(f"üì¶ Remaining: {total_batches - batch_num - 1} batches ({(total_batches - batch_num - 1) * 100} properties)")
            input("\nüëâ Press ENTER to process next batch (or Ctrl+C to stop)...")
        else:
            print(f"\nüéâ All {total_batches} batches processed!")
    
    # Combine all batch files into one final file
    print("\n" + "="*80)
    print("üì¶ COMBINING ALL BATCHES INTO FINAL REPORT")
    print("="*80)
    
    all_results = []
    for batch_num in range(total_batches):
        batch_file = f'data/results/buyer_analysis_batch_{batch_num + 1}_{timestamp}.csv'
        if os.path.exists(batch_file):
            batch_df = pd.read_csv(batch_file)
            all_results.append(batch_df)
    
    if all_results:
        final_df = pd.concat(all_results, ignore_index=True)
        
        # Remove any duplicates that might have slipped through
        before_dedup = len(final_df)
        if 'Property_ID' in final_df.columns:
            final_df = final_df.drop_duplicates(subset=['Property_ID'], keep='first')
        else:
            final_df = final_df.drop_duplicates(subset=['BHK', 'Area_SqFt', 'Locality', 'Price_Lakhs'], keep='first')
        after_dedup = len(final_df)
        
        final_file = f'data/results/buyer_focused_analysis_complete_{timestamp}.csv'
        final_df.to_csv(final_file, index=False, encoding='utf-8-sig')
        
        print(f"\n‚úÖ Final Report Created!")
        print(f"üìä Total Properties: {len(final_df)}")
        if before_dedup > after_dedup:
            print(f"   üßπ Removed {before_dedup - after_dedup} duplicate entries")
        print(f"üîß Processing: Advanced NLP Analysis")
        print(f"üíæ Complete Report: {final_file}")
        print(f"\nüìã Report includes:")
        print("   ‚Ä¢ Property overview & investment analysis")
        print("   ‚Ä¢ Location advantages & target buyer profile")
        print("   ‚Ä¢ Complete amenities & features analysis")
        print("   ‚Ä¢ Quality scores & ratings")
        print("   ‚Ä¢ Locality insights & market positioning")
        print("   ‚Ä¢ Investment recommendations")
    
    print("\n" + "="*80)

def process_quick_analysis():
    """Quick analysis of entire dataset"""
    print("\n" + "="*80)
    print("QUICK ANALYSIS - ENTIRE DATASET")
    print("="*80)
    
    # Load dataset
    print("\nüìÇ Loading dataset...")
    try:
        df = pd.read_csv('data/cleaned/cleaned_data.csv')
        df = df[df['Locality'] != 'Unknown']
        print(f"‚úÖ Loaded {len(df)} properties")
    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        return
    
    # Initialize modules
    extractor = AmenityExtractor()
    scorer = DescriptionQualityScorer()
    
    results = []
    
    print(f"\n‚è≥ Processing {len(df)} properties (quick mode)...")
    print("‚ö†Ô∏è  Estimated time: 3-5 minutes\n")
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing"):
        prop_data = row.to_dict()
        
        try:
            description = str(prop_data.get('Description', ''))
            amenities = extractor.extract_amenities(description)
            scores = scorer.calculate_overall_score(prop_data, description)
            
            result = {
                'Property_ID': idx,
                'BHK': prop_data.get('BHK'),
                'Area_SqFt': prop_data.get('Area_SqFt'),
                'Locality': prop_data.get('Locality'),
                'Price_Lakhs': prop_data.get('Price_Lakhs'),
                'Property_Type': prop_data.get('Property_Type'),
                'Amenities_Count': len(amenities),
                'Amenities_List': ', '.join(amenities),
                'Quality_Score': scores['overall_score'],
                'Score_Out_Of_100': scores['score_out_of_100'],
                'Rating': scores['rating']
            }
            
            results.append(result)
            
        except Exception as e:
            continue
    
    # Save results
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'data/results/quick_analysis_{timestamp}.csv'
    
    df_results = pd.DataFrame(results)
    df_results.to_csv(output_file, index=False)
    
    print(f"\n‚úÖ Quick analysis complete!")
    print(f"üìä Processed: {len(results)} properties")
    print(f"üíæ Results saved to: {output_file}")
    print("\n" + "="*80)

def process_locality_analysis():
    """Process locality-level analysis"""
    print("\n" + "="*80)
    print("LOCALITY ANALYSIS - ALL AREAS")
    print("="*80)
    
    analyzer = LocalityAnalyzer()
    
    if analyzer.df.empty:
        print("‚ùå No data available")
        return
    
    # Get all localities
    localities = analyzer.df['Locality'].unique()
    print(f"\nüìç Found {len(localities)} localities")
    
    results = []
    
    print(f"\n‚è≥ Analyzing {len(localities)} localities...\n")
    
    for locality in tqdm(localities, desc="Processing"):
        try:
            stats = analyzer.get_locality_stats(locality)
            personality = analyzer.determine_locality_personality(locality)
            target_audience = analyzer.identify_target_audience(locality)
            
            result = {
                'Locality': locality,
                'Property_Count': stats['property_count'],
                'Avg_Price_Lakhs': stats['avg_price_lakhs'],
                'Median_Price_Lakhs': stats['median_price_lakhs'],
                'Min_Price_Lakhs': stats['min_price_lakhs'],
                'Max_Price_Lakhs': stats['max_price_lakhs'],
                'Avg_Area_SqFt': stats['avg_area_sqft'],
                'Locality_Tier': stats['locality_tier'],
                'Personality': personality['personality'],
                'Description': personality['description'],
                'Tags': ', '.join(personality['tags']),
                'Target_Audience': ', '.join(target_audience)
            }
            
            results.append(result)
            
        except Exception as e:
            continue
    
    # Save results
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'data/results/locality_analysis_{timestamp}.csv'
    
    df_results = pd.DataFrame(results)
    df_results.to_csv(output_file, index=False)
    
    print(f"\n‚úÖ Locality analysis complete!")
    print(f"üìä Analyzed: {len(results)} localities")
    print(f"üíæ Results saved to: {output_file}")
    print("\n" + "="*80)

def view_results():
    """View existing results"""
    print("\n" + "="*80)
    print("VIEWING RESULTS")
    print("="*80)
    
    results_dir = 'data/results'
    
    if not os.path.exists(results_dir):
        print("\n‚ùå No results directory found")
        return
    
    # List all CSV files
    csv_files = [f for f in os.listdir(results_dir) if f.endswith('.csv')]
    
    if not csv_files:
        print("\n‚ùå No result files found")
        print("üí° Run processing options (1-3) first to generate results")
        return
    
    print(f"\nüìÅ Found {len(csv_files)} result files:\n")
    for i, file in enumerate(csv_files, 1):
        file_path = os.path.join(results_dir, file)
        size = os.path.getsize(file_path) / 1024  # KB
        print(f"  {i}. {file} ({size:.1f} KB)")
    
    choice = input("\nüëâ Enter file number to view summary (or Enter to skip): ").strip()
    
    if choice.isdigit() and 1 <= int(choice) <= len(csv_files):
        file_path = os.path.join(results_dir, csv_files[int(choice)-1])
        df = pd.read_csv(file_path)
        
        print(f"\nüìä Summary of {csv_files[int(choice)-1]}:")
        print(f"  Total rows: {len(df)}")
        print(f"  Columns: {len(df.columns)}")
        print(f"\nüìã Column names:")
        for col in df.columns:
            print(f"    ‚Ä¢ {col}")
        
        print(f"\nüìà Sample data (first 3 rows):")
        print(df.head(3).to_string())

def launch_chatbot():
    """Launch the property chatbot"""
    print("\n" + "="*80)
    print("  LAUNCHING PROPERTY CHATBOT...")
    print("="*80)
    
    try:
        import subprocess
        subprocess.run([sys.executable, 'chatbot.py'])
    except Exception as e:
        print(f"\n‚ùå Error launching chatbot: {e}")
        print("\nüí° You can run it manually:")
        print("   python chatbot.py")

def main():
    """Main Phase 2 interface"""
    print("\n" + "="*80)
    print("  REALESTATESENSE - PHASE 2: OLLAMA-POWERED NLP ENGINE")
    print("="*80)
    print("\n  üè† Using LOCAL Ollama (100% Private, No API needed)")
    print("  üìä Complete Investment & Locality Analysis")
    print("  üí¨ Interactive AI Chatbot for Dataset Q&A")
    print("  üí° Buyer-Focused Comprehensive Reports")
    print("\n" + "="*80)
    
    while True:
        print_menu()
        choice = input("üëâ Enter your choice (0-3): ").strip()
        
        if choice == '0':
            print("\nüëã Goodbye!")
            break
        elif choice == '1':
            confirm = input("\n‚ö†Ô∏è  This will process properties with Ollama AI. Continue? (y/n): ")
            if confirm.lower() == 'y':
                process_entire_dataset()
        elif choice == '2':
            launch_chatbot()
        elif choice == '3':
            view_results()
        else:
            print("\n‚ùå Invalid choice! Please enter 0-3")
        
        input("\n‚è∏Ô∏è  Press Enter to continue...")

if __name__ == "__main__":
    main()
